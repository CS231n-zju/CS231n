{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import pickle\n",
    "def load_CIFAR_batch(filename):\n",
    "\n",
    "    with open(filename,'rb') as f:\n",
    "        datadict= pickle.load(f,encoding='bytes')\n",
    "        X=datadict[b'data']\n",
    "        Y=datadict[b'labels']\n",
    "        X=X.reshape(10000,3,32,32).transpose(0,2,3,1).astype(\"float\")\n",
    "        Y=np.array(Y)\n",
    "        return X,Y\n",
    "\n",
    "def load_CIFAR10(root):\n",
    "\n",
    "    xs=[]\n",
    "    ys=[]\n",
    "    for b in range(1,6):\n",
    "        f=os.path.join(root,'data_batch_%d'%(b,))\n",
    "        X,Y=load_CIFAR_batch(f)\n",
    "        xs.append(X)\n",
    "        ys.append(Y)\n",
    "    Xtr=np.concatenate(xs)\n",
    "    Ytr=np.concatenate(ys)\n",
    "    del X,Y\n",
    "    Xte,Yte=load_CIFAR_batch(os.path.join(root,'test_batch'))\n",
    "    return Xtr,Ytr,Xte,Yte\n",
    "\n",
    "def load_tiny_imagenet(path, dtype=np.float32):\n",
    "    with open(os.path.join(path, 'wnids.txt'), 'r') as f:\n",
    "        wnids = [x.strip() for x in f]\n",
    "\n",
    "    wnid_to_label = {wnid: i for i, wnid in enumerate(wnids)}\n",
    "\n",
    "    with open(os.path.join(path, 'words.txt'), 'r') as f:\n",
    "        wnid_to_words = dict(line.split('\\t') for line in f)\n",
    "        for wnid, words in wnid_to_words.iteritems():\n",
    "            wnid_to_words[wnid] = [w.strip() for w in words.split(',')]\n",
    "    class_names = [wnid_to_words[wnid] for wnid in wnids]\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i, wnid in enumerate(wnids):\n",
    "        if (i + 1) % 20 == 0:\n",
    "            print ('loading training data for synset %d / %d' % (i + 1, len(wnids)))\n",
    "        # To figure out the filenames we need to open the boxes file\n",
    "        boxes_file = os.path.join(path, 'train', wnid, '%s_boxes.txt' % wnid)\n",
    "        with open(boxes_file, 'r') as f:\n",
    "            filenames = [x.split('\\t')[0] for x in f]\n",
    "        num_images = len(filenames)\n",
    "\n",
    "        X_train_block = np.zeros((num_images, 3, 64, 64), dtype=dtype)\n",
    "        y_train_block = wnid_to_label[wnid] * np.ones(num_images, dtype=np.int64)\n",
    "        for j, img_file in enumerate(filenames):\n",
    "            img_file = os.path.join(path, 'train', wnid, 'images', img_file)\n",
    "            img = imread(img_file)\n",
    "            if img.ndim == 2:\n",
    "                ## grayscale file\n",
    "                img.shape = (64, 64, 1)\n",
    "            X_train_block[j] = img.transpose(2, 0, 1)\n",
    "        X_train.append(X_train_block)\n",
    "        y_train.append(y_train_block)\n",
    "\n",
    "    # We need to concatenate all training data\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "    # Next load validation data\n",
    "    with open(os.path.join(path, 'val', 'val_annotations.txt'), 'r') as f:\n",
    "        img_files = []\n",
    "        val_wnids = []\n",
    "        for line in f:\n",
    "            img_file, wnid = line.split('\\t')[:2]\n",
    "            img_files.append(img_file)\n",
    "            val_wnids.append(wnid)\n",
    "        num_val = len(img_files)\n",
    "        y_val = np.array([wnid_to_label[wnid] for wnid in val_wnids])\n",
    "        X_val = np.zeros((num_val, 3, 64, 64), dtype=dtype)\n",
    "        for i, img_file in enumerate(img_files):\n",
    "            img_file = os.path.join(path, 'val', 'images', img_file)\n",
    "            img = imread(img_file)\n",
    "            if img.ndim == 2:\n",
    "                img.shape = (64, 64, 1)\n",
    "            X_val[i] = img.transpose(2, 0, 1)\n",
    "\n",
    "    # Next load test images\n",
    "    # Students won't have test labels, so we need to iterate over files in the\n",
    "    # images directory.\n",
    "    img_files = os.listdir(os.path.join(path, 'test', 'images'))\n",
    "    X_test = np.zeros((len(img_files), 3, 64, 64), dtype=dtype)\n",
    "    for i, img_file in enumerate(img_files):\n",
    "        img_file = os.path.join(path, 'test', 'images', img_file)\n",
    "        img = imread(img_file)\n",
    "        if img.ndim == 2:\n",
    "            img.shape = (64, 64, 1)\n",
    "        X_test[i] = img.transpose(2, 0, 1)\n",
    "\n",
    "    y_test = None\n",
    "    y_test_file = os.path.join(path, 'test', 'test_annotations.txt')\n",
    "    if os.path.isfile(y_test_file):\n",
    "        with open(y_test_file, 'r') as f:\n",
    "            img_file_to_wnid = {}\n",
    "            for line in f:\n",
    "                line = line.split('\\t')\n",
    "                img_file_to_wnid[line[0]] = line[1]\n",
    "        y_test = [wnid_to_label[img_file_to_wnid[img_file]] for img_file in img_files]\n",
    "        y_test = np.array(y_test)\n",
    "\n",
    "    return class_names, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def load_models(models_dir):\n",
    "    models = {}\n",
    "    for model_file in os.listdir(models_dir):\n",
    "        with open(os.path.join(models_dir, model_file), 'rb') as f:\n",
    "            try:\n",
    "                models[model_file] = pickle.load(f)['model']\n",
    "            except pickle.UnpicklingError:\n",
    "                continue\n",
    "    return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize']=(10.0,8.0)\n",
    "plt.rcParams['image.interpolation']='nearest'\n",
    "plt.rcParams['image.cmap']='gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\nTrain labels shape:  (49000,)\nValidation data shape:  (1000, 3073)\nValidation labels shape:  (1000,)\nTest data shape:  (1000, 3073)\nTest labels shape:  (1000,)\ndev data shape:  (500, 3073)\ndev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'I:\\Workplace\\PycharmProjects\\cs231n\\Assignment1\\cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "\n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "\n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "    loss=0.0\n",
    "    dW=np.zeros(W.shape)\n",
    "    N,C=X.shape[0],W.shape[1]\n",
    "    for i in range(N):\n",
    "        f=np.dot(X[i],W)\n",
    "        f-=np.max(f)\n",
    "        loss=loss+np.log(np.sum(np.exp(f)))-f[y[i]]\n",
    "        dW[:,y[i]]-=X[i]\n",
    "        s=np.exp(f).sum()\n",
    "        for j in range(C):\n",
    "            dW[:,j]+=np.exp(f[j])/s*X[i]\n",
    "    loss=loss/N+0.5*reg*np.sum(W*W)\n",
    "    dW=dW/N+reg*W\n",
    "    return loss,dW\n",
    "\n",
    "def softmax_loss_vectorized(W,X,y,reg):\n",
    "    loss=0.0\n",
    "    dW=np.zeros_like(W)\n",
    "    N=X.shape[0]\n",
    "    f=np.dot(X,W)\n",
    "    f-=f.max(axis=1).reshape(N,1)\n",
    "    s=np.exp(f).sum(axis=1)\n",
    "    loss=np.log(s).sum()-f[range(W),y].sum()\n",
    "    \n",
    "    counts=np.exp(f)/s.reshape(N,1)\n",
    "    counts[range(N),y]-=1\n",
    "    dW=np.dot(X.T,counts)\n",
    "    loss=loss/N+0.5*reg*np.sum(W*W)\n",
    "    dW=dW/N+reg*W\n",
    "    return loss,dW\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.29549070032\nsanity check: 2.30258509299\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "W=np.random.rand(3073,10)*0.0001\n",
    "loss,grad=softmax_loss_naive(W,X_dev,y_dev,0.0)\n",
    "print('loss:', loss)\n",
    "print('sanity check:',(-np.log(0.1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:utf-8\n",
    "import numpy as np\n",
    "from random import randrange\n",
    "\n",
    "\n",
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "    fx = f(x)\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        fxph = f(x)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x)\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h)\n",
    "        if verbose:\n",
    "            print(ix, grad[ix])\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n",
    "    numeric_diffs = []\n",
    "    for input_blob in inputs:\n",
    "        diff = np.zeros_like(input_blob.diffs)\n",
    "        it = np.nditer(input_blob.vals, flags=['multi_index'],\n",
    "                       op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            orig = input_blob.vals[idx]\n",
    "\n",
    "            input_blob.vals[idx] = orig + h\n",
    "            f(*(inputs + (output,)))\n",
    "            pos = np.copy(output.vals)\n",
    "            input_blob.vals[idx] = orig - h\n",
    "            f(*(inputs + (output,)))\n",
    "            neg = np.copy(output.vals)\n",
    "            input_blob.vals[idx] = orig\n",
    "\n",
    "            diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n",
    "\n",
    "            it.iternext()\n",
    "        numeric_diffs.append(diff)\n",
    "    return numeric_diffs\n",
    "\n",
    "\n",
    "def eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n",
    "    return eval_numerical_gradient_blobs(lambda *args: net.forward(), inputs, output, h=h)\n",
    "\n",
    "\n",
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
    "    for i in range(num_checks):\n",
    "        ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h  # increment by h\n",
    "        fxph = f(x)  # evaluate f(x + h)\n",
    "        x[ix] = oldval - h  # increment by h\n",
    "        fxmh = f(x)  # evaluate f(x - h)\n",
    "        x[ix] = oldval  # reset\n",
    "\n",
    "        grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "        print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.269572 analytic: 1.269572, relative error: 1.887087e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -2.131754 analytic: -2.131754, relative error: 2.799809e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.224761 analytic: 2.224761, relative error: 2.852523e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.102133 analytic: 2.102133, relative error: 7.923065e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 4.440443 analytic: 4.440443, relative error: 4.298982e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.232405 analytic: 2.232405, relative error: 4.072212e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.227717 analytic: 1.227717, relative error: 5.805273e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.134141 analytic: -1.134141, relative error: 2.548290e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.258481 analytic: -1.258481, relative error: 2.269035e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.640507 analytic: 2.640506, relative error: 2.311581e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.067370 analytic: 0.067370, relative error: 4.530563e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.548080 analytic: -1.548080, relative error: 4.631937e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.990269 analytic: -0.990269, relative error: 6.263535e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.473911 analytic: 0.473911, relative error: 2.083714e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.994285 analytic: 1.994285, relative error: 2.940083e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.790198 analytic: -0.790199, relative error: 5.611005e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.033621 analytic: 1.033621, relative error: 6.432515e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 4.638344 analytic: 4.638344, relative error: 5.975018e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 2.674102 analytic: 2.674102, relative error: 1.533204e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.502520 analytic: 0.502520, relative error: 8.256605e-08\n"
     ]
    }
   ],
   "source": [
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
